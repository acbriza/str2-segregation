{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb9b9822",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Segregation dynamics with reinforcement learning and agent based modeling\n",
    "\n",
    "**Sert, Egemen, Yaneer Bar-Yam, and Alfredo J. Morales.  *Scientific reports* 10.1 (2020): 1-12.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d81d55d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PRELIMINARY CONCEPTS\n",
    "- **Agent Based Modeling (ABM)**\n",
    "    - a generative approach to study natural phenomena based on the interaction of individuals in social, physical and biological systems\n",
    "- **Reinforcement Learning** \n",
    "    - a simulation method where agents become intelligent and create new, optimal behaviors based on a previously defined structure of rewards and the state of their environment\n",
    "- **Multiagent Reinforcement Learning**\n",
    "    - employs multiple agents\n",
    "- **Schelling's Segregation Model**\n",
    "    - Demonstrates that individual preferences to live away from those that are different may sort social systems in the large scale and generate patterns of social segregation without the need of centralized enforcement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b2905f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RESEARCH GAP\n",
    "- Previous studies using ABM tackled segregation and cases of integration ...\n",
    "- But they were unable to explore a wide space of possible behaviours based on different types rewards\n",
    "    - Rewards are key to understand people's choices and decisions. Thus, it seems logical to incorporate them into ABMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d30d29",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CONTRIBUTIONS\n",
    "- Adapt Schelling's model to RL\n",
    "- Combine RL with ABM to explore self-organizing dynamics of social segregation and explore space of probabilities by considering different types of rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646359f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## METHOD \n",
    "\n",
    "#### Part 1: RL Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe25a20",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### General Idea\n",
    "- Explore varying levels of segregation and integration rewards and observe the effects on the dynamics of the system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92a3a07",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Grid Environment\n",
    "- 50 x 50 with periodic boundary conditions (wrapped environment)\n",
    "\n",
    "<img src='grid.png' style=\"width: 250px\"></img>\n",
    "<justify><small>*Grid world of experiments. The grid size is 50×50 locations. Red and blue squares denote the two types of agents respectively. White cells\n",
    "represents empty regions.*</small></justify>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddac1f9e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Agents\n",
    "- 2 types: A & B   \n",
    "- Has an $n$ x $n$ observation window\n",
    "    - $ n = 2r + 1$\n",
    "    - $n=11$; $r=5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa7f429",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### State\n",
    "- Consists of Spatial observation and Age observation\n",
    "- Spatial observation ($o_{spatial}$)\n",
    "    - Values in the agent's observation window (agent at the center)\n",
    "        - Possible values are {1,0,-1} corresponding to (self/friend, empty, foe)\n",
    "- Age observation ($o_{age}$)\n",
    "    - Agent's remaining normalized life time\n",
    "- State space is $O(M3^{n^2})$, if there are $M$ age values  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6959d5c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Actions\n",
    "- 5 possible actions\n",
    "    - stay still, left, right, up or down (vs. Schelling where you can move anywhere)\n",
    "- Agents take 1 action per iteration\n",
    "    - Sequence of agent who will take action is taken randomly\n",
    "- Agent lives for a maximum of 100 iterations\n",
    "- When an agent dies, a new agent is born in a random location\n",
    "- Agents extend their lifespan by interacting with agents of the opposite kind\n",
    "    - Interaction happens when an agent moves to a location of another agent of different kind\n",
    "    - Agent who moved is given reward plus lifespan extension; loser dies\n",
    "        - Possible interpretation: emigration of the losing agent out of the neighborhood    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8d8bd0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Rewards\n",
    "- $R = SR + IR + VR + DR + OR + TR$\n",
    "- Segregation Reward (SR)\n",
    "    - $SR =   s − \\alpha d$\n",
    "    - Promotes segregation \n",
    "    - $s$ agents of same kind; $d$ agents of different kind; $\\alpha$ [0,1]\n",
    "    - $\\alpha$ is intolerance to agents of different kind (higher $\\alpha$, more intolerant)\n",
    "- Interdependence Reward (IR)\n",
    "    - [0,100]\n",
    "- Vigilance Reward (VR)\n",
    "    - 0.1 for each time step an agent remains alive\n",
    "    - Encourages staying alive \n",
    "- Death Reward (DR)\n",
    "    - -1 if agent die; 0 if remains alive\n",
    "- Occlusion Reward (OR)\n",
    "    - -1 if you occupy same area of same kind; else 0\n",
    "- Stillness Reward (TR)\n",
    "    - -1 if action=remain still, else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbf44dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## METHOD \n",
    "\n",
    "#### Part 2: Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52d0f5f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Network Architecture\n",
    "- 2 NN for agent type A and B (Fig 1 bottom)\n",
    "    - for a competitive multi-agent RL environment\n",
    "<img src='network_architecture.png' style=\"width: 750px\"></img>\n",
    "<justify><small>*Each type of agent has its own Deep Q-Network. Every agent has a field of view of 11\n",
    "×11 locations. Green border denotes the field of view of the agent illustrated in green. Agents can move acrossempty spaces.Two models are created for $φ_A$ and $φ_B$ respectively. Each network receives an input of 11×11 locations, runs it through five convolution steps and concatenates the resulting activations with the agent’s remaining age normalized by the maximum initial age. The feature vector is mapped over the action space using a fully connected layer. The action with the maximum Q-value is taken for the agent.*</small></justify>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f97c41",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reinforcement Learning\n",
    "- Let $φ_A$ and $φ_B$ denote the **Deep Q-Networks** of type A and B agents. The goal of these networks is to satisfy the following:\n",
    "<img src='network_objective.png' style=\"width: 300px\"></img>\n",
    "- $N_A$ and $N_B$ are the number of type A and B agents\n",
    "- $\\gamma$ is the discount factor\n",
    "- $r_t$ is the reward at time $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce01176a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Experiment Parameters\n",
    "- Exploration via $\\epsilon$-greedy strategy\n",
    "- Optimizer: Adam\n",
    "- Uses Experience Replay to mitigate time correlation among the inputs of NN\n",
    "- Runs 1 episode per experiment \n",
    "- Each experiment has 5000 iterations\n",
    "- Each experiment repeated 10 times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b423065",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='parameters.png' style=\"width: 300px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1675a69",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae435d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Segregation\n",
    "<img src='segregation.png' style=\"width: 850px\"></img>\n",
    "***Notes*** \n",
    "- averaged over 1000 iterations\n",
    "- $\\alpha$ is intolerance to agents of different kind (higher $\\alpha$, more intolerant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f6a2c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Observations on Iteration lengths (Panel A)\n",
    "- Legend\n",
    "    - red - dominated by A agents\n",
    "    - blue dominated by blue B agents\n",
    "    - white - average pattern or mixed population for small $\\alpha$\n",
    "        - empty for higher $\\alpha$\n",
    "- Observation\n",
    "    - Lower $\\alpha$: mixed population; as $\\alpha$ increases, segregation happens \n",
    "        - happens even in first 1K iterations\n",
    "    - Similar to Schelling, segregation still happens for in the long run smaller alpha  (e.g. 0.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73efa3be",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='segregation.png' style=\"width: 850px\"></img>\n",
    "<justify><small>*Agents collective behavior for multiple values of segregation reward $\\alpha$ (rows) at multiple times (columns).*</small></justify>\n",
    "\n",
    "***Notes*** \n",
    "- averaged over 1000 iterations\n",
    "- $\\alpha$ is intolerance to agents of different kind (higher $\\alpha$, more intolerant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96562b82",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Observations on Agent Age (Panel B)\n",
    "- Legend\n",
    "    - red - older average age\n",
    "    - blue - younger average age\n",
    "- Observation\n",
    "    - Low  $\\alpha$, low mixing of age; higher  $\\alpha$, higher mixing of age\n",
    "    - White intercluster regions have low average age\n",
    "    - Segregated clusters -older ones inside, younger ones in periphery\n",
    "       - Meaning, there's little interaction across all agents (same or different type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4e4a43",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Multiscale entropy\n",
    "- Higher levels of alpha achieved higher levels of segregation faster\n",
    "- Lower levels of alpha don't reach equilibrium unlike Schelling model bec. agents are always seeking  reward\n",
    "\n",
    "<img src='entropy.png' style=\"width: 550px\"></img>\n",
    "<justify><small>*Segregation dynamics for multiple values of segregation reward (α). The curves are\n",
    "obtained by averaging 50 iterations over 10 experiment realizations. Shades denote the standard deviation across experiments.*</small></justify>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b309d288",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interdependencies\n",
    "\n",
    "<img src='interdependence.png' style=\"width: 850px\"></img>\n",
    "<justify><small>*Agents collective behavior for multiple values of interdependence reward (IR) at multiple times (columns) for maximum segregation parameter ($\\alpha$ = 1).*</small></justify>\n",
    "\n",
    "***Notes*** \n",
    "- averaged over 1000 iterations\n",
    "- $IR$ promotes interactions and create interdependecies among populations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50db3986",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Observations on Iteration lengths (Panel A)\n",
    "- Observation\n",
    "    - $IR$=0 segregation immediately result\n",
    "    - As $IR$ increases, areas become uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5256bd9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Segregation and Interdependence\n",
    "\n",
    "<img src='segregation_interdependence.png' style=\"width: 300px\"></img>\n",
    "\n",
    "- Red means high segregation, blue lower segregation\n",
    "- Segregation is high when promoted and $IR$ is low\n",
    "- As $IR$ increase, agents mix even for high values of alpha\n",
    "- Conclusion: High values of $IR$ counter the rewards fro segregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5844c9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Age Dynamics\n",
    "\n",
    "<img src='age_grid.png' style=\"width: 800px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5150a35",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Red higher probability of finding an age group at a given level of segregation\n",
    "- Older agents have significantly more segregated observation windows than younger agents (more red)\n",
    "    - more pronounced for lower values of $IR$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dd06a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Biases of Actions\n",
    "\n",
    "<img src='action_grid.png' style=\"width: 800px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cab1d4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Certain movements are biased towards certain age groups\n",
    "    - Older agents tend to stay more still  \n",
    "    - Younger agents seem to explore the space further\n",
    "- Reason: for older agents, rewards for other social interactions are lower than staying safe. \n",
    "- This behavior has been verified with human behavior using Census data across the US. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7272a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CRITIQUE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164c8ea1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decisions and Tricks\n",
    "- Low density of agents (greater density may result in less learning)\n",
    "    - 5% of each agent type wrt size of grid\n",
    "- maximum age = 100\n",
    "- $IR$ seems to be big vs. $SR$ (factors of 25 for $IR$ vs max 12 for $SR$)\n",
    "- Use DQN instead of QLearning because of the size of state space\n",
    "- 2 netwoks for a competitive multi-agent RL environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb68a624",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Unsupported Claims\n",
    "- Weak claim: \"We believe  older agents become more segregated because the expected rewards for other social interactions are lower than staying safe.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f21067",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fit with other Papers\n",
    "- Original Schelling\n",
    "-  ABM tackled segregation and cases of integration but without exploring rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a0581a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ideas I disagree with \n",
    "- Agent who moved is given reward (lifespan extension); loser dies\n",
    "    - Can the interaction be non-hostile? (E.g. interaction is when you move beside an agent of a different kind)\n",
    "- Why allow 2 agents to co-locate \n",
    "- Why not combine Vigilance and Death rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8414c33f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Points for clarification\n",
    "- In order to homogenize the networks’ inputs, we normalize the observation windows by the agents’ own kind, such that positive and negative values respectively represent equal and opposite kind for each agent.\n",
    "- Older agents become more segregated because the expected rewards for other social interactions are lower than staying safe. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdbf03f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Possible Experiments\n",
    "- What 'breaks' am I looking for?\n",
    "    - if segregation does not happen when...\n",
    "        - IR is not hostile\n",
    "- ABM side\n",
    "    - Tweak interdependency definition \n",
    "        - No death by interaction\n",
    "        - Don't allow occlusion/co-location\n",
    "    - Tweak number of agents\n",
    "    - Agents' age\n",
    "    - White is empty or mixed?  New plot or coloring\n",
    "        - **Differentiate**\n",
    "- RL side\n",
    "    - Play with other rewards\n",
    "        - Occlusion\n",
    "        - Stillness\n",
    "    - Age as part of RL?\n",
    "- Segregation Dynamics (Fig 3)\n",
    "    - Experiment on default: 50 iterations over 10 experimental realizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ba61c7",
   "metadata": {},
   "source": [
    "#### Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2567e58d",
   "metadata": {},
   "source": [
    "- Step 1 reproduce results and graphs\n",
    "    - Each experiment has 5000 iterations\n",
    "    - Each experiment repeated 10 times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3004a39b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## QUESTIONS?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
